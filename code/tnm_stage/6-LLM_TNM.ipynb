{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook uses an LLM to perform TNM staging classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/common/lopezgg/miniconda3/envs/vllm_2/lib/python3.12/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.generics:GenericModel` has been moved to `pydantic.BaseModel`.\n",
            "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n",
            "/common/lopezgg/miniconda3/envs/vllm_2/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 06-16 21:22:15 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "\n",
        "from enum import Enum\n",
        "from pydantic import BaseModel\n",
        "from pydantic.generics import GenericModel\n",
        "from typing import TypeVar, Generic, Any\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import utils\n",
        "import llm_utils\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from vllm import LLM, SamplingParams\n",
        "from vllm.sampling_params import GuidedDecodingParams\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model\n",
        "model_size = \"8B\"\n",
        "model_name = f\"/common/gonzalezg7lab/Projects/LLMs/externalModels/Meta-Llama-3.1-{model_size}-Instruct\"\n",
        "\n",
        "data_dir = \"../data/mimic_corpus/test.csv\"\n",
        "prompt_path = \"./prompts/\"\n",
        "out_path = f\"./model_preds/tnm_label_llama_{model_size}_prompt_{{prompt}}_{{dataset}}_preds.csv\"\n",
        "prompt_path = \"./prompts\"\n",
        "\n",
        "max_input_len = 90000\n",
        "max_output_len = 1024\n",
        "\n",
        "cuda_gpu_id = \"0\"\n",
        "\n",
        "data_dir = \"../../data/tnm_stage\"\n",
        "\n",
        "dict_tnm_labels = {\n",
        "    \"t_label\": [\"T1\", \"T2\", \"T3\", \"T4\"],\n",
        "    \"n_label\": [\"N0\", \"N1\", \"N2\", \"N3\"],\n",
        "    \"m_label\": [\"M0\", \"M1\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of GPUs available: 1\n"
          ]
        }
      ],
      "source": [
        "if cuda_gpu_id != \"-1\":\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_gpu_id\n",
        "_ = torch.device('cuda')\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "assert torch.cuda.is_available()\n",
        "print(\"Number of GPUs available:\", torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8WsLF10gjltK"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(os.path.join(data_dir, \"train_tcga_reports_tnm_stage.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1947, 6)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert not df_train['patient_id'].duplicated().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.index = df_train['patient_id'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8WsLF10gjltK"
      },
      "outputs": [],
      "source": [
        "df_val = pd.read_csv(os.path.join(data_dir, \"val_tcga_reports_tnm_stage.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(780, 6)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert not df_val['patient_id'].duplicated().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_val.index = df_val['patient_id'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8WsLF10gjltK"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(os.path.join(data_dir, \"test_tcga_reports_tnm_stage.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1170, 6)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert not df_test['patient_id'].duplicated().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test.index = df_test['patient_id'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_name = \"tnm_zs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_msg_template = \"\"\"Pathology report: \"{text}\"\n",
        "Output in JSON format: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Structured output JSON format\n",
        "class T_Value(str, Enum):\n",
        "    t1 = \"T1\"\n",
        "    t2 = \"T2\"\n",
        "    t3 = \"T3\"\n",
        "    t4 = \"T4\"\n",
        "\n",
        "\n",
        "class N_Value(str, Enum):\n",
        "    n0 = \"N0\"\n",
        "    n1 = \"N1\"\n",
        "    n2 = \"N2\"\n",
        "    n3 = \"N3\"\n",
        "\n",
        "\n",
        "class M_Value(str, Enum):\n",
        "    m0 = \"M0\"\n",
        "    m1 = \"M1\"\n",
        "\n",
        "\n",
        "# Generic type variable\n",
        "E = TypeVar(\"E\", T_Value, N_Value, M_Value)\n",
        "\n",
        "\n",
        "# Generic TNM item model\n",
        "class TNM_Item(GenericModel, Generic[E]):\n",
        "    explanation: Any\n",
        "    label: E\n",
        "\n",
        "\n",
        "# Final TNM format model\n",
        "class TNM_Format(BaseModel):\n",
        "    T: TNM_Item[T_Value]\n",
        "    N: TNM_Item[N_Value]\n",
        "    M: TNM_Item[M_Value]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "arr_prompt = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': \"\"\"You are a medical expert tasked with extracting cancer stage information from pathology reports. Based on the text provided, classify the tumor stage (T), lymph node involvement (N), and metastasis (M) according to standard TNM classification guidelines. Provide both a classification label and a brief natural language explanation for each category. If information is not explicit, infer the most likely value based on the context.\n",
        "\n",
        "- T can take on 4 integer values: 1 if tumor is 2cm or less across, 2 if tumor is more than 2cm but not more than 5cm across, 3 if tumor is more than 5cm across, or 4 if tumor of any size growing into the chest wall or skin. Return no text beyond the integer value.\n",
        "- N can take on 4 integer values: 0 if there are no cancer cells in any nearby nodes or only small clusters of cancer cells less than 0.2 mm across, 1 if there are cancer cells in 1 to 3 lymph nodes, 2 if there are 4 to 9 lymph nodes in the armpit and at least one is larger than 2 mm, or 3 if there are cancer cells in 10 or more lymph nodes in the armpit and at least one is larger than 2 mm.\"\n",
        "- M can take on 2 integer values: 0 if there is no distant metastasis, or 1 if there is distant metastasis (i.e. cancer that has spread from the original (primary) tumor to distant organs or distant lymph nodes).\"\n",
        "\n",
        "\n",
        "Return your answer strictly in the following JSON format:\n",
        "\n",
        "{\n",
        "  \"T\": {\n",
        "    \"explanation\": [Your reasoning for T classification],\n",
        "    \"label\": [T classification label]\n",
        "  },\n",
        "  \"N\": {\n",
        "    \"explanation\": [Your reasoning for N classification],\n",
        "    \"label\": [N classification label]\n",
        "  },\n",
        "  \"M\": {\n",
        "    \"explanation\": [Your reasoning for M classification],\n",
        "    \"label\": [M classification label]\n",
        "  }\n",
        "}\"\"\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the prompt\n",
        "with open(os.path.join(prompt_path, f\"{prompt_name}.json\"), 'w') as f:\n",
        "    json.dump(arr_prompt, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first analyze the token length of each document in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "arr_input_prompt = llm_utils.create_prompt(\n",
        "    df_eval=pd.DataFrame({'text': df_train['text'].to_list() + df_val['text'].to_list() + df_test['text'].to_list()}),\n",
        "    func_format_user=llm_utils.func_format_user,\n",
        "    messages=arr_prompt,\n",
        "    user_template=user_msg_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "arr_tok_prompt = [\n",
        "    tokenizer.apply_chat_template(\n",
        "        prompt,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True\n",
        "    )\n",
        "    for prompt in arr_input_prompt\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "count    3897.000000\n",
            "mean     1317.801642\n",
            "std       825.997221\n",
            "min       463.000000\n",
            "25%       680.000000\n",
            "50%      1074.000000\n",
            "75%      1677.000000\n",
            "max      5934.000000\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "arr_tok_len = pd.Series([len(seq) for seq in arr_tok_prompt])\n",
        "print(arr_tok_len.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       abs  rel\n",
            "True  3897  1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(pd.DataFrame({\n",
        "    \"abs\": (arr_tok_len <= max_input_len).value_counts(normalize=False),\n",
        "    \"rel\": (arr_tok_len <= max_input_len).value_counts(normalize=True)\n",
        "}))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All documents fit into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading  /common/gonzalezg7lab/Projects/LLMs/externalModels/Meta-Llama-3.1-8B-Instruct\n",
            "INFO 06-16 21:22:32 [config.py:717] This model supports multiple tasks: {'classify', 'score', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\n",
            "INFO 06-16 21:22:32 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/common/lopezgg/miniconda3/envs/vllm_2/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 06-16 21:22:37 [__init__.py:239] Automatically detected platform cuda.\n",
            "INFO 06-16 21:22:39 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/common/gonzalezg7lab/Projects/LLMs/externalModels/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='/common/gonzalezg7lab/Projects/LLMs/externalModels/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=90000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/common/gonzalezg7lab/Projects/LLMs/externalModels/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
            "WARNING 06-16 21:22:39 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x155459f8c9b0>\n",
            "INFO 06-16 21:22:40 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 06-16 21:22:40 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
            "WARNING 06-16 21:22:40 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 06-16 21:22:40 [gpu_model_runner.py:1329] Starting to load model /common/gonzalezg7lab/Projects/LLMs/externalModels/Meta-Llama-3.1-8B-Instruct...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.37it/s]\n",
            "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.39it/s]\n",
            "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.07it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.79it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.73it/s]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 06-16 21:22:42 [loader.py:458] Loading weights took 2.37 seconds\n",
            "INFO 06-16 21:22:43 [gpu_model_runner.py:1347] Model loading took 14.9889 GiB and 2.577817 seconds\n",
            "INFO 06-16 21:22:49 [backends.py:420] Using cache directory: /home/lopezgg/.cache/vllm/torch_compile_cache/6fefe9b3ea/rank_0_0 for vLLM's torch.compile\n",
            "INFO 06-16 21:22:49 [backends.py:430] Dynamo bytecode transform time: 6.10 s\n",
            "INFO 06-16 21:22:53 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 3.821 s\n",
            "INFO 06-16 21:22:54 [monitor.py:33] torch.compile takes 6.10 s in total\n",
            "INFO 06-16 21:22:54 [kv_cache_utils.py:634] GPU KV cache size: 517,696 tokens\n",
            "INFO 06-16 21:22:54 [kv_cache_utils.py:637] Maximum concurrency for 90,000 tokens per request: 5.75x\n",
            "INFO 06-16 21:23:08 [gpu_model_runner.py:1686] Graph capturing finished in 14 secs, took 0.64 GiB\n",
            "INFO 06-16 21:23:08 [core.py:159] init engine (profile, create kv cache, warmup model) took 25.52 seconds\n",
            "INFO 06-16 21:23:08 [core_client.py:439] Core engine process 0 ready.\n",
            "Model loaded!\n",
            "Total loading time: 0:00:43.311010\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading \", model_name)\n",
        "start_time = time.time()\n",
        "model = LLM(\n",
        "    model=model_name,\n",
        "    tensor_parallel_size=number_gpus,\n",
        "    dtype=torch.bfloat16,\n",
        "    gpu_memory_utilization=.90,\n",
        "    max_model_len=max_input_len\n",
        ")\n",
        "end_time = time.time()\n",
        "print(\"Model loaded!\")\n",
        "print(\"Total loading time:\", str(timedelta(seconds=end_time - start_time)))\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampling_params = SamplingParams(\n",
        "    temperature=.6,\n",
        "    top_p=.9,\n",
        "    max_tokens=max_output_len,\n",
        "    seed=0,\n",
        "    detokenize=True,\n",
        "    guided_decoding=GuidedDecodingParams(\n",
        "        json=TNM_Format.model_json_schema()\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "arr_val_input_prompt = llm_utils.create_prompt(\n",
        "    df_eval=df_val,\n",
        "    func_format_user=llm_utils.func_format_user,\n",
        "    messages=arr_prompt,\n",
        "    user_template=user_msg_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of texts to predict: 780\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of texts to predict:\", len(arr_val_input_prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "566049fbd1db4d49822f22986c2db31b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/780 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "arr_val_text_pred = llm_utils.eval_prompt(\n",
        "    arr_input_prompt=arr_val_input_prompt,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    sampling_params=sampling_params\n",
        ")\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total inference time: 0:00:56.959879\n"
          ]
        }
      ],
      "source": [
        "print(\"Total inference time:\", str(timedelta(seconds=end_time - start_time)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_val_pred = llm_utils.extract_preds(\n",
        "    df_eval=df_val,\n",
        "    arr_preds=arr_val_text_pred\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "tnm_label = \"t_label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5448717948717948"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(\n",
        "    y_true=df_val[tnm_label].values,\n",
        "    y_pred=df_val_pred[tnm_label].values\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>n_train</th>\n",
              "      <th>n_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>T1</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.457143</td>\n",
              "      <td>435</td>\n",
              "      <td>175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>T2</td>\n",
              "      <td>0.541528</td>\n",
              "      <td>0.592727</td>\n",
              "      <td>0.565972</td>\n",
              "      <td>689</td>\n",
              "      <td>275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>T3</td>\n",
              "      <td>0.489362</td>\n",
              "      <td>0.766667</td>\n",
              "      <td>0.597403</td>\n",
              "      <td>596</td>\n",
              "      <td>240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>T4</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.244444</td>\n",
              "      <td>0.357724</td>\n",
              "      <td>227</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  precision    recall        f1  n_train  n_val\n",
              "0    T1   0.800000  0.320000  0.457143      435    175\n",
              "1    T2   0.541528  0.592727  0.565972      689    275\n",
              "2    T3   0.489362  0.766667  0.597403      596    240\n",
              "3    T4   0.666667  0.244444  0.357724      227     90"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "utils.calculate_performance(\n",
        "    arr_gs=df_val[tnm_label].values,\n",
        "    arr_preds=df_val_pred[tnm_label].values,\n",
        "    arr_labels=dict_tnm_labels[tnm_label],\n",
        "    col_label=tnm_label,\n",
        "    df_data=df_val,\n",
        "    df_train_data=df_train\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "tnm_label = \"n_label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8320512820512821"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(\n",
        "    y_true=df_val[tnm_label].values,\n",
        "    y_pred=df_val_pred[tnm_label].values\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>n_train</th>\n",
              "      <th>n_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>N0</td>\n",
              "      <td>0.955056</td>\n",
              "      <td>0.938190</td>\n",
              "      <td>0.946548</td>\n",
              "      <td>1129</td>\n",
              "      <td>453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>N1</td>\n",
              "      <td>0.815029</td>\n",
              "      <td>0.701493</td>\n",
              "      <td>0.754011</td>\n",
              "      <td>503</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>N2</td>\n",
              "      <td>0.559633</td>\n",
              "      <td>0.648936</td>\n",
              "      <td>0.600985</td>\n",
              "      <td>236</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>N3</td>\n",
              "      <td>0.415094</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.517647</td>\n",
              "      <td>79</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  precision    recall        f1  n_train  n_val\n",
              "0    N0   0.955056  0.938190  0.946548     1129    453\n",
              "1    N1   0.815029  0.701493  0.754011      503    201\n",
              "2    N2   0.559633  0.648936  0.600985      236     94\n",
              "3    N3   0.415094  0.687500  0.517647       79     32"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "utils.calculate_performance(\n",
        "    arr_gs=df_val[tnm_label].values,\n",
        "    arr_preds=df_val_pred[tnm_label].values,\n",
        "    arr_labels=dict_tnm_labels[tnm_label],\n",
        "    col_label=tnm_label,\n",
        "    df_data=df_val,\n",
        "    df_train_data=df_train\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "tnm_label = \"m_label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9230769230769231"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(\n",
        "    y_true=df_val[tnm_label].values,\n",
        "    y_pred=df_val_pred[tnm_label].values\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>n_train</th>\n",
              "      <th>n_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M0</td>\n",
              "      <td>0.965326</td>\n",
              "      <td>0.952120</td>\n",
              "      <td>0.958678</td>\n",
              "      <td>1821</td>\n",
              "      <td>731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M1</td>\n",
              "      <td>0.406780</td>\n",
              "      <td>0.489796</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>126</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  precision    recall        f1  n_train  n_val\n",
              "0    M0   0.965326  0.952120  0.958678     1821    731\n",
              "1    M1   0.406780  0.489796  0.444444      126     49"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "utils.calculate_performance(\n",
        "    arr_gs=df_val[tnm_label].values,\n",
        "    arr_preds=df_val_pred[tnm_label].values,\n",
        "    arr_labels=dict_tnm_labels[tnm_label],\n",
        "    col_label=tnm_label,\n",
        "    df_data=df_val,\n",
        "    df_train_data=df_train\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We save the model predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_val_pred.to_csv(\n",
        "    out_path.format(\n",
        "        prompt=prompt_name,\n",
        "        dataset=\"val\"\n",
        "    ),\n",
        "    index=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "arr_test_input_prompt = llm_utils.create_prompt(\n",
        "    df_eval=df_test,\n",
        "    func_format_user=llm_utils.func_format_user,\n",
        "    messages=arr_prompt,\n",
        "    user_template=user_msg_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of texts to predict: 1170\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of texts to predict:\", len(arr_test_input_prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc8ffe3e633a4e6680acb232c719c87e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/1170 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "arr_test_text_pred = llm_utils.eval_prompt(\n",
        "    arr_input_prompt=arr_test_input_prompt,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    sampling_params=sampling_params\n",
        ")\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total inference time: 0:01:22.879696\n"
          ]
        }
      ],
      "source": [
        "print(\"Total inference time:\", str(timedelta(seconds=end_time - start_time)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test_pred = llm_utils.extract_preds(\n",
        "    df_eval=df_test,\n",
        "    arr_preds=arr_test_text_pred\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "tnm_label = \"t_label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.541025641025641"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(\n",
        "    y_true=df_test[tnm_label].values,\n",
        "    y_pred=df_test_pred[tnm_label].values\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>n_train</th>\n",
              "      <th>n_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>T1</td>\n",
              "      <td>0.788889</td>\n",
              "      <td>0.270992</td>\n",
              "      <td>0.403409</td>\n",
              "      <td>435</td>\n",
              "      <td>262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>T2</td>\n",
              "      <td>0.525727</td>\n",
              "      <td>0.570388</td>\n",
              "      <td>0.547148</td>\n",
              "      <td>689</td>\n",
              "      <td>412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>T3</td>\n",
              "      <td>0.484429</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.597015</td>\n",
              "      <td>596</td>\n",
              "      <td>360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>T4</td>\n",
              "      <td>0.854545</td>\n",
              "      <td>0.345588</td>\n",
              "      <td>0.492147</td>\n",
              "      <td>227</td>\n",
              "      <td>136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  precision    recall        f1  n_train  n_val\n",
              "0    T1   0.788889  0.270992  0.403409      435    262\n",
              "1    T2   0.525727  0.570388  0.547148      689    412\n",
              "2    T3   0.484429  0.777778  0.597015      596    360\n",
              "3    T4   0.854545  0.345588  0.492147      227    136"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "utils.calculate_performance(\n",
        "    arr_gs=df_test[tnm_label].values,\n",
        "    arr_preds=df_test_pred[tnm_label].values,\n",
        "    arr_labels=dict_tnm_labels[tnm_label],\n",
        "    col_label=tnm_label,\n",
        "    df_data=df_test,\n",
        "    df_train_data=df_train\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "tnm_label = \"n_label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8427350427350427"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(\n",
        "    y_true=df_test[tnm_label].values,\n",
        "    y_pred=df_test_pred[tnm_label].values\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>n_train</th>\n",
              "      <th>n_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>N0</td>\n",
              "      <td>0.959762</td>\n",
              "      <td>0.948454</td>\n",
              "      <td>0.954074</td>\n",
              "      <td>1129</td>\n",
              "      <td>679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>N1</td>\n",
              "      <td>0.846473</td>\n",
              "      <td>0.675497</td>\n",
              "      <td>0.751381</td>\n",
              "      <td>503</td>\n",
              "      <td>302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>N2</td>\n",
              "      <td>0.562842</td>\n",
              "      <td>0.725352</td>\n",
              "      <td>0.633846</td>\n",
              "      <td>236</td>\n",
              "      <td>142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>N3</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.744681</td>\n",
              "      <td>0.573770</td>\n",
              "      <td>79</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  precision    recall        f1  n_train  n_val\n",
              "0    N0   0.959762  0.948454  0.954074     1129    679\n",
              "1    N1   0.846473  0.675497  0.751381      503    302\n",
              "2    N2   0.562842  0.725352  0.633846      236    142\n",
              "3    N3   0.466667  0.744681  0.573770       79     47"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "utils.calculate_performance(\n",
        "    arr_gs=df_test[tnm_label].values,\n",
        "    arr_preds=df_test_pred[tnm_label].values,\n",
        "    arr_labels=dict_tnm_labels[tnm_label],\n",
        "    col_label=tnm_label,\n",
        "    df_data=df_test,\n",
        "    df_train_data=df_train\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "tnm_label = \"m_label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9085470085470085"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(\n",
        "    y_true=df_test[tnm_label].values,\n",
        "    y_pred=df_test_pred[tnm_label].values\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>n_train</th>\n",
              "      <th>n_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M0</td>\n",
              "      <td>0.966070</td>\n",
              "      <td>0.935219</td>\n",
              "      <td>0.950394</td>\n",
              "      <td>1821</td>\n",
              "      <td>1096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M1</td>\n",
              "      <td>0.348624</td>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.415301</td>\n",
              "      <td>126</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  precision    recall        f1  n_train  n_val\n",
              "0    M0   0.966070  0.935219  0.950394     1821   1096\n",
              "1    M1   0.348624  0.513514  0.415301      126     74"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "utils.calculate_performance(\n",
        "    arr_gs=df_test[tnm_label].values,\n",
        "    arr_preds=df_test_pred[tnm_label].values,\n",
        "    arr_labels=dict_tnm_labels[tnm_label],\n",
        "    col_label=tnm_label,\n",
        "    df_data=df_test,\n",
        "    df_train_data=df_train\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We save the model predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test_pred.to_csv(\n",
        "    out_path.format(\n",
        "        prompt=prompt_name,\n",
        "        dataset=\"test\"\n",
        "    ),\n",
        "    index=False\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
